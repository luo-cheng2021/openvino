
# auto generated by ir2py from /home/luocheng/models/gpt2_test/origin.xml

from openvino.runtime import Core, Model, Tensor, PartialShape, Type, Shape, serialize
from openvino.runtime.op import util as op_util
from openvino.runtime import opset10 as opset
from openvino.runtime.passes import Manager
import numpy as np
import sys, os
def const(data, itype):
    if isinstance(data, list):
        shape = [len(data)]
    else:
        data = [data]
        shape = []
    return opset.constant(itype, Shape(shape), data)
def const_i64(data):
    return const(data, Type.i64)
def const_i32(data):
    return const(data, Type.i32)
def collect_const(model, consts):
    for n in model.get_ordered_ops():
        if n.get_type_name() == "Constant":
            # print(n.get_friendly_name())
            consts[n.get_friendly_name()] = list(n.get_vector())
        if hasattr(n, "get_function"):
            collect_const(n.get_function(), consts)
    return
def show_io(m):
    print("Inputs of the model:")
    for port, _input in enumerate(m.inputs):
        print("	[{}] {}".format(port, _input))
    print("Outputs of the model:")
    for port, _output in enumerate(m.outputs):
        print("	[{}] {}".format(port, _output))

LAYER_NUM = 2 #32
HEAD_NUM = 12 #32
SIZE_PER_HEAD = 64 #80
HIDDEN_SIZE = HEAD_NUM * SIZE_PER_HEAD
INTERMEDIATE_SIZE = 1024 #10240
LAYER_NORM_EPS = 1e-5
MAX_POSITION_EMBEDDINGS = 1024 #2048
ROTARY_EMB_BASE = 10000
ROTARY_PCT = 0.25
USE_PARALLEL_RESIDUAL = True
VOCAB_SIZE = 50304
MAX_SEQ_LEN = 400
FakeConstDict = {
    'model.gpt_neox.embed_in.weight': np.zeros((VOCAB_SIZE, HIDDEN_SIZE), dtype=np.float32),
    'model.gpt_neox.embed_out.weight': np.zeros((HIDDEN_SIZE, VOCAB_SIZE), dtype=np.float32),
    'model.gpt_neox.layers.attention.query_key_value.bias': [
        np.zeros((HIDDEN_SIZE * 3,), dtype=np.float32)
    ] * LAYER_NUM,
    'model.gpt_neox.layers.attention.query_key_value.weight': [
        np.zeros((HIDDEN_SIZE, HIDDEN_SIZE * 3), dtype=np.float32)
    ] * LAYER_NUM,
    'model.gpt_neox.layers.attention.dense.bias': [
        np.zeros((HIDDEN_SIZE,), dtype=np.float32)
    ] * LAYER_NUM,
    'model.gpt_neox.layers.attention.dense.weight': [
        np.zeros((HIDDEN_SIZE, HIDDEN_SIZE), dtype=np.float32)
    ] * LAYER_NUM,
    'model.gpt_neox.layers.mlp.dense_h_to_4h.bias': [
        np.zeros((INTERMEDIATE_SIZE,), dtype=np.float32)
    ] * LAYER_NUM,
    'model.gpt_neox.layers.mlp.dense_h_to_4h.weight': [
        np.zeros((HIDDEN_SIZE, INTERMEDIATE_SIZE), dtype=np.float32)
    ] * LAYER_NUM,
    'model.gpt_neox.layers.mlp.dense_4h_to_h.bias': [
        np.zeros((HIDDEN_SIZE,), dtype=np.float32)
    ] * LAYER_NUM,
    'model.gpt_neox.layers.mlp.dense_4h_to_h.weight': [
        np.zeros((INTERMEDIATE_SIZE, HIDDEN_SIZE), dtype=np.float32)
    ] * LAYER_NUM,
}
def layer(hidden_states, past_key_values, past_keys_num, layer_idx, ConstDict):
    input_layernorm = opset.mvn(hidden_states, axes=[1], normalize_variance=True, eps=LAYER_NORM_EPS, eps_mode="inside_sqrt", name=f'/model/gpt_neox/layers.{layer_idx}/attention/layer_norm')
    ######### attention part begin
    query_key_value_bias = opset.constant(ConstDict['model.gpt_neox.layers.attention.query_key_value.bias'][layer_idx], Type.f32, name=f'model.gpt_neox.layers.{layer_idx}.attention.query_key_value.bias')
    query_key_value_weights = opset.constant(ConstDict['model.gpt_neox.layers.attention.query_key_value.weight'][layer_idx], Type.f32, name='model.gpt_neox.layers.attention.query_key_value.weight')
    qkv_ = opset.matmul(input_layernorm, query_key_value_weights, transpose_a=False, transpose_b=False, name=f'/model/gpt_neox/layers.{layer_idx}/attention/query_key_value/MatMul') #wildcard [?,?,7680]
    qkv = opset.add(query_key_value_bias, qkv_, auto_broadcast='numpy', name=f'/model/gpt_neox/layers.{layer_idx}/attention/query_key_value/Add') #wildcard [?,?,7680]

    # custom op
    attn_output = opset.gpt_neox_attn(qkv, past_key_values, past_keys_num,
            layer_num=LAYER_NUM, head_num=HEAD_NUM, size_per_head=SIZE_PER_HEAD, hidden_size=HIDDEN_SIZE, intermediate_size=INTERMEDIATE_SIZE, layer_norm_eps=LAYER_NORM_EPS, max_position_embeddings=MAX_POSITION_EMBEDDINGS,
            rotary_emb_base=ROTARY_EMB_BASE, rotary_pct=ROTARY_PCT, use_parallel_residual=USE_PARALLEL_RESIDUAL, vocab_size=VOCAB_SIZE, max_seq_len=MAX_SEQ_LEN, cur_layer_num=layer_idx, name=f'/model/gpt_neox/layers.{layer_idx}/attention/attn')

    # attn_output = self.dense(attn_output) line: 157
    dense_weight = opset.constant(ConstDict['model.gpt_neox.layers.attention.dense.weight'][layer_idx], Type.f32, name=f'model.gpt_neox.layers.{layer_idx}.attention.dense.weight')
    dense_ = opset.matmul(attn_output, dense_weight, transpose_a=False,transpose_b=False, name=f'/model/gpt_neox/layers.{layer_idx}/attention/dense/MatMul') #wildcard [?,?,HIDDEN_SIZE]
    dense_bias = opset.constant(ConstDict['model.gpt_neox.layers.attention.dense.bias'][layer_idx], Type.f32, name=f'model.gpt_neox.layers.{layer_idx}.attention.dense.bias')
    dense = opset.add(dense_bias, dense_, auto_broadcast='numpy', name=f'/model/gpt_neox/layers.{layer_idx}/attention/dense/Add') #wildcard [?,?,HIDDEN_SIZE]
    attn_output = dense
    ######### attention part end
    # use_parallel_residual
    post_attention_layernorm = opset.mvn(attn_output, axes=[1], normalize_variance=True, eps=LAYER_NORM_EPS, eps_mode="inside_sqrt", name=f'/model/gpt_neox/layers.{layer_idx}/post_attention_layernorm')
    # mlp
    def mlp(states):
        dense_h_to_4h_bias = opset.constant(ConstDict['model.gpt_neox.layers.mlp.dense_h_to_4h.bias'][layer_idx], Type.f32, name=f'model.gpt_neox.layers.{layer_idx}.mlp.dense_h_to_4h.bias')
        dense_h_to_4h_weight = opset.constant(ConstDict['model.gpt_neox.layers.mlp.dense_h_to_4h.weight'][layer_idx], Type.f32, name=f'model.gpt_neox.layers.{layer_idx}.mlp.dense_h_to_4h.weight')
        dense_h_to_4h_ = opset.matmul(states,dense_h_to_4h_weight, transpose_a=False,transpose_b=False, name=f'/model/gpt_neox/layers.{layer_idx}/mlp/dense_h_to_4h/MatMul') #wildcard [?,?,INTERMEDIATE_SIZE]
        dense_h_to_4h = opset.add(dense_h_to_4h_bias,dense_h_to_4h_, auto_broadcast='numpy', name=f'/model/gpt_neox/layers.{layer_idx}/mlp/dense_h_to_4h/Add') #wildcard [?,?,INTERMEDIATE_SIZE]
        gelu = opset.gelu(dense_h_to_4h, approximation_mode='erf', name=f'/model/gpt_neox/layers.{layer_idx}/mlp/dense_h_to_4h/Gelu')
        dense_4h_to_h_bias = opset.constant(ConstDict['model.gpt_neox.layers.mlp.dense_4h_to_h.bias'][layer_idx], Type.f32, name=f'model.gpt_neox.layers.{layer_idx}.mlp.dense_4h_to_h.bias')
        dense_4h_to_h_weight = opset.constant(ConstDict['model.gpt_neox.layers.mlp.dense_4h_to_h.weight'][layer_idx], Type.f32, name=f'model.gpt_neox.layers.{layer_idx}.mlp.dense_4h_to_h.weight')
        dense_4h_to_h_ = opset.matmul(gelu,dense_4h_to_h_weight, transpose_a=False,transpose_b=False, name=f'/model/gpt_neox/layers.{layer_idx}/mlp/dense_4h_to_h/MatMul') #wildcard [?,?,HIDDEN_SIZE]
        dense_4h_to_h = opset.add(dense_4h_to_h_bias,dense_4h_to_h_, auto_broadcast='numpy', name=f'/model/gpt_neox/layers.{layer_idx}/mlp/dense_4h_to_h/Add') #wildcard [?,?,HIDDEN_SIZE]
        return dense_4h_to_h

    mlp_output = mlp(post_attention_layernorm)
    hidden_states_tmp = opset.add(mlp_output, attn_output)
    return opset.add(hidden_states_tmp, hidden_states)

def create_model(arg):
    if isinstance(arg, Model):
        ConstDict = {}
        collect_const(arg, ConstDict)
    else:
        ConstDict = arg

    input_ids = opset.parameter([-1, -1], Type.i64, name='input_ids')
    past_key_values = opset.parameter([LAYER_NUM, 2, -1, HEAD_NUM, MAX_SEQ_LEN, SIZE_PER_HEAD], Type.f32, name='past_key_values')
    past_keys_num = opset.parameter([1,], Type.i64, name='past_keys_num')

    embed_in_const = opset.constant(ConstDict['model.gpt_neox.embed_in.weight'], Type.f32)
    inputs_embeds = opset.gather(embed_in_const, indices=input_ids, axis=0) # name='/model/gpt_neox/embed_in/Gather') # [?,?,HIDDEN_SIZE]
    hidden_states = inputs_embeds
    for i in range(LAYER_NUM):
        hidden_states = layer(hidden_states, past_key_values, past_keys_num, i, ConstDict)
    # final_layer_norm
    final_layer_norm = opset.mvn(hidden_states, axes=[1], normalize_variance=True, eps=LAYER_NORM_EPS, eps_mode="inside_sqrt", name='/model/gpt_neox/final_layer_norm')
    # embed_out
    embed_out_weight = opset.constant(ConstDict['model.gpt_neox.embed_out.weight'], Type.f32)
    embed_out = opset.matmul(final_layer_norm,embed_out_weight, transpose_a=False,transpose_b=False, name='logits') #wildcard [?,?,VOCAB_SIZE]
    embed_out_result = opset.result(embed_out, name='logits/sink_port_0') # [?,?,VOCAB_SIZE]
    return Model([embed_out_result], [input_ids, past_key_values, past_keys_num])

if __name__ == "__main__":
    # # org_model_path = "/home/luocheng/models/gpt2_test/origin.xml"
    # # core = Core()
    # # # example of introducing custome (OP) extension
    # # # import os
    # # # os.environ["add_RnntUpdate_opset8"] = "1"
    # # # core.add_extension("/home/dev/tingqian/ov-rnnt/rnnt_ov_extension/build/librnnt_ov_extension.so")
    # # model = core.read_model(org_model_path)
    # # print("====", org_model_path)
    # # show_io(model)
    model2 = create_model(FakeConstDict)
    print("====", "new model")
    show_io(model2)
    serialize(model2, "./hacked/gpt_neox.xml")

